import math
from collections import Counter

# Dataset: [feature1, feature2, label] â€” binary classification
data = [
    [2.7, 2.5, 0],
    [1.3, 1.5, 0],
    [3.0, 3.5, 0],
    [7.6, 8.0, 1],
    [9.0, 8.5, 1],
    [8.2, 9.1, 1]
]

X = [row[:-1] for row in data]
y = [row[-1] for row in data]

# Test sample
test = [3.5, 3.0]

# ----------------- Naive Bayes ----------------- #
def naive_bayes_predict(X, y, sample):
    summaries = {}
    for label in set(y):
        features = [x for x, l in zip(X, y) if l == label]
        means = [sum(f[i] for f in features) / len(features) for i in range(len(sample))]
        variances = [sum((f[i] - means[i])**2 for f in features) / len(features) for i in range(len(sample))]
        summaries[label] = (means, variances, len(features)/len(X))

    probs = {}
    for label, (mean, var, prior) in summaries.items():
        prob = math.log(prior)
        for i in range(len(sample)):
            eps = 1e-2
            coeff = 1 / math.sqrt(2 * math.pi * (var[i] + eps))
            exp = math.exp(-(sample[i] - mean[i]) ** 2 / (2 * (var[i] + eps)))
            prob += math.log(coeff * exp)
        probs[label] = prob
    return max(probs, key=probs.get)

# ----------------- K-Nearest Neighbors ----------------- #
def knn_predict(X, y, sample, k=3):
    distances = [ (math.dist(sample, xi), yi) for xi, yi in zip(X, y)]
    distances.sort()
    k_nearest = [label for _, label in distances[:k]]
    return Counter(k_nearest).most_common(1)[0][0]

# ----------------- Simple Decision Tree (One Level) ----------------- #
def decision_tree_predict(X, y, sample):
    # Use only one feature to split: pick the one with best separation
    best_accuracy = 0
    best_feature = 0
    best_threshold = 0
    for i in range(len(X[0])):  # for each feature
        thresholds = sorted(set(x[i] for x in X))
        for t in thresholds:
            preds = [0 if x[i] <= t else 1 for x in X]
            correct = sum(p == l for p, l in zip(preds, y))
            acc = correct / len(y)
            if acc > best_accuracy:
                best_accuracy = acc
                best_feature = i
                best_threshold = t
    return 0 if sample[best_feature] <= best_threshold else 1

# ----------------- Run All ----------------- #
nb = naive_bayes_predict(X, y, test)
knn = knn_predict(X, y, test)
dt = decision_tree_predict(X, y, test)

print("Test Sample:", test)
print("Naive Bayes Prediction:", nb)
print("K-NN Prediction:", knn)
print("Decision Tree Prediction:", dt)
