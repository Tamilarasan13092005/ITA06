import numpy as np

# Iris subset data: features + labels
data = np.array([
    [5.1, 3.5, 1.4, 0.2, 0],
    [4.9, 3.0, 1.4, 0.2, 0],
    [6.2, 2.9, 4.3, 1.3, 1],
    [5.6, 2.9, 3.6, 1.3, 1],
    [6.3, 3.3, 6.0, 2.5, 2],
    [5.8, 2.7, 5.1, 1.9, 2]
])

X = data[:, :-1]
y = data[:, -1].astype(int)
classes = np.unique(y)

def summarize_by_class(X, y):
    eps = 1e-2  # smoothing to add to variance
    summaries = {}
    for c in classes:
        X_c = X[y == c]
        mean = X_c.mean(axis=0)
        var = X_c.var(axis=0) + eps  # add smoothing here
        prior = X_c.shape[0] / X.shape[0]
        summaries[c] = {'mean': mean, 'var': var, 'prior': prior}
    return summaries

def gaussian_pdf(x, mean, var):
    coeff = 1.0 / np.sqrt(2.0 * np.pi * var)
    exponent = np.exp(-(x - mean) ** 2 / (2 * var))
    return coeff * exponent

def predict(sample, summaries):
    posteriors = {}
    for c, stats in summaries.items():
        prior = np.log(stats['prior'])
        conditional = np.sum(np.log(gaussian_pdf(sample, stats['mean'], stats['var'])))
        posteriors[c] = prior + conditional
    return max(posteriors, key=posteriors.get)

# Train model
summaries = summarize_by_class(X, y)

# Test sample
test_sample = np.array([6.0, 3.0, 4.8, 1.8])
predicted_class = predict(test_sample, summaries)

class_names = {0: "Setosa", 1: "Versicolor", 2: "Virginica"}
print(f"Predicted class for sample {test_sample}: {class_names[predicted_class]}")
